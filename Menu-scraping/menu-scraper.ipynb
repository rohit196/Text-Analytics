{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary Libraries\n",
    "#%pip install webdriver_manager fake_useragent\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import random\n",
    "import argparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from fake_useragent import UserAgent\n",
    "import platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h]\n",
      "                             [--restaurants RESTAURANTS [RESTAURANTS ...]]\n",
      "                             [--output-dir OUTPUT_DIR] [--no-headless]\n",
      "                             [--proxy PROXY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/codespace/.local/share/jupyter/runtime/kernel-v312fc60deb9edae1b941be18e1510f1b849327102.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "# Set up logging\n",
    "log_directory = \"logs\"\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "log_file = os.path.join(log_directory, f\"menu_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MenuScraper:\n",
    "    def __init__(self, headless=True, use_proxy=False, proxy=None):\n",
    "        self.use_proxy = use_proxy\n",
    "        self.proxy = proxy\n",
    "        self.headless = headless\n",
    "        self.setup_driver()\n",
    "        self.user_agent = UserAgent()\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Set up the Selenium WebDriver with Chrome\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            if self.headless:\n",
    "                chrome_options.add_argument(\"--headless\")\n",
    "            \n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "            chrome_options.add_argument(\"--disable-notifications\")\n",
    "            chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "            chrome_options.add_argument(f\"user-agent={self.user_agent.random}\")\n",
    "            \n",
    "            if self.use_proxy and self.proxy:\n",
    "                chrome_options.add_argument(f'--proxy-server={self.proxy}')\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            logger.info(\"WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error setting up WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def random_delay(self, min_sec=1, max_sec=5):\n",
    "        \"\"\"Add a random delay to mimic human behavior and avoid detection\"\"\"\n",
    "        delay = random.uniform(min_sec, max_sec)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def scrape_aw(self):\n",
    "        \"\"\"Scrape A&W menu\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting A&W menu scraping\")\n",
    "            menu_url = \"https://web.aw.ca/en/our-menu\"\n",
    "            self.driver.get(menu_url)\n",
    "            \n",
    "            # Wait for menu items to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".menu-category-container\"))\n",
    "            )\n",
    "            \n",
    "            # Allow time for dynamic content to load\n",
    "            self.random_delay(2, 4)\n",
    "            \n",
    "            # Scroll down to load all content\n",
    "            self.scroll_page()\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            menu_data = []\n",
    "            \n",
    "            # Extract menu categories\n",
    "            menu_categories = soup.select(\".menu-category-container\")\n",
    "            logger.info(f\"Found {len(menu_categories)} menu categories\")\n",
    "            \n",
    "            for category in menu_categories:\n",
    "                # Get category name\n",
    "                category_title_elem = category.select_one(\".menu-category-title\")\n",
    "                category_name = category_title_elem.get_text(strip=True) if category_title_elem else \"Uncategorized\"\n",
    "                logger.info(f\"Processing category: {category_name}\")\n",
    "                \n",
    "                # Extract menu items for this category\n",
    "                menu_items = category.select(\".menu-item\")\n",
    "                for item in menu_items:\n",
    "                    # Extract item name\n",
    "                    item_name_elem = item.select_one(\".menu-item-title\")\n",
    "                    item_name = item_name_elem.get_text(strip=True) if item_name_elem else \"Unknown\"\n",
    "                    \n",
    "                    # Extract item description\n",
    "                    item_desc_elem = item.select_one(\".menu-item-desc\")\n",
    "                    item_description = item_desc_elem.get_text(strip=True) if item_desc_elem else \"\"\n",
    "                    \n",
    "                    # Extract price (A&W might not show prices directly on the menu page)\n",
    "                    item_price_elem = item.select_one(\".menu-item-price\")\n",
    "                    item_price = item_price_elem.get_text(strip=True) if item_price_elem else \"Price not available online\"\n",
    "                    \n",
    "                    # Extract image URL if available\n",
    "                    item_image = \"\"\n",
    "                    img_tag = item.select_one(\"img\")\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        item_image = img_tag.get('src')\n",
    "                        \n",
    "                        # If it's a relative URL, make it absolute\n",
    "                        if item_image.startswith('/'):\n",
    "                            item_image = f\"https://web.aw.ca{item_image}\"\n",
    "                    \n",
    "                    # Add nutritional info if available\n",
    "                    nutritional_info = {}\n",
    "                    nutrition_elem = item.select_one(\".nutrition-info\")\n",
    "                    if nutrition_elem:\n",
    "                        nutrition_items = nutrition_elem.select(\".nutrition-item\")\n",
    "                        for n_item in nutrition_items:\n",
    "                            key_elem = n_item.select_one(\".nutrition-key\")\n",
    "                            value_elem = n_item.select_one(\".nutrition-value\")\n",
    "                            if key_elem and value_elem:\n",
    "                                key = key_elem.get_text(strip=True)\n",
    "                                value = value_elem.get_text(strip=True)\n",
    "                                nutritional_info[key] = value\n",
    "                    \n",
    "                    menu_data.append({\n",
    "                        \"restaurant\": \"A&W\",\n",
    "                        \"category\": category_name,\n",
    "                        \"name\": item_name,\n",
    "                        \"price\": item_price,\n",
    "                        \"description\": item_description,\n",
    "                        \"image_url\": item_image,\n",
    "                        \"nutritional_info\": nutritional_info\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Scraped {len(menu_data)} items from A&W menu\")\n",
    "            return menu_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping A&W menu: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_mcdonalds(self):\n",
    "        \"\"\"Scrape McDonald's menu\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting McDonald's menu scraping\")\n",
    "            menu_url = \"https://www.mcdonalds.com/us/en-us/full-menu.html\"\n",
    "            self.driver.get(menu_url)\n",
    "            \n",
    "            # Wait for menu items to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".category-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            # Allow time for dynamic content to load\n",
    "            self.random_delay(2, 4)\n",
    "            \n",
    "            # Scroll down to load all content\n",
    "            self.scroll_page()\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            menu_data = []\n",
    "            \n",
    "            # Extract menu categories\n",
    "            menu_categories = soup.select(\".category-wrapper\")\n",
    "            logger.info(f\"Found {len(menu_categories)} menu categories\")\n",
    "            \n",
    "            for category in menu_categories:\n",
    "                category_name = category.select_one(\"h2\").get_text(strip=True) if category.select_one(\"h2\") else \"Uncategorized\"\n",
    "                logger.info(f\"Processing category: {category_name}\")\n",
    "                \n",
    "                # Extract menu items for this category\n",
    "                menu_items = category.select(\".cmp-category-item\")\n",
    "                for item in menu_items:\n",
    "                    item_name = item.select_one(\".item-title\").get_text(strip=True) if item.select_one(\".item-title\") else \"Unknown\"\n",
    "                    item_price = item.select_one(\".item-price\").get_text(strip=True) if item.select_one(\".item-price\") else \"N/A\"\n",
    "                    item_description = item.select_one(\".item-description\").get_text(strip=True) if item.select_one(\".item-description\") else \"\"\n",
    "                    item_image = \"\"\n",
    "                    \n",
    "                    img_tag = item.select_one(\"img\")\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        item_image = img_tag.get('src')\n",
    "                    \n",
    "                    menu_data.append({\n",
    "                        \"restaurant\": \"McDonald's\",\n",
    "                        \"category\": category_name,\n",
    "                        \"name\": item_name,\n",
    "                        \"price\": item_price,\n",
    "                        \"description\": item_description,\n",
    "                        \"image_url\": item_image\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Scraped {len(menu_data)} items from McDonald's menu\")\n",
    "            return menu_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping McDonald's menu: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_burger_king(self):\n",
    "        \"\"\"Scrape Burger King menu\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting Burger King menu scraping\")\n",
    "            menu_url = \"https://www.bk.com/menu\"\n",
    "            self.driver.get(menu_url)\n",
    "            \n",
    "            # Wait for menu items to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".menuPage_menuCategory__Qbda1\"))\n",
    "            )\n",
    "            \n",
    "            # Allow time for dynamic content to load\n",
    "            self.random_delay(2, 4)\n",
    "            \n",
    "            # Scroll down to load all content\n",
    "            self.scroll_page()\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            menu_data = []\n",
    "            \n",
    "            # Extract menu categories\n",
    "            menu_categories = soup.select(\".menuPage_menuCategory__Qbda1\")\n",
    "            logger.info(f\"Found {len(menu_categories)} menu categories\")\n",
    "            \n",
    "            for category in menu_categories:\n",
    "                category_name = category.select_one(\"h2\").get_text(strip=True) if category.select_one(\"h2\") else \"Uncategorized\"\n",
    "                logger.info(f\"Processing category: {category_name}\")\n",
    "                \n",
    "                # Extract menu items for this category\n",
    "                menu_items = category.select(\".menuItem_wrapper__X_zY_\")\n",
    "                for item in menu_items:\n",
    "                    item_name = item.select_one(\".menuItem_name__on_cM\").get_text(strip=True) if item.select_one(\".menuItem_name__on_cM\") else \"Unknown\"\n",
    "                    item_price = item.select_one(\".menuItem_price__TPsSC\").get_text(strip=True) if item.select_one(\".menuItem_price__TPsSC\") else \"N/A\"\n",
    "                    item_description = item.select_one(\".menuItem_description__i5zkV\").get_text(strip=True) if item.select_one(\".menuItem_description__i5zkV\") else \"\"\n",
    "                    item_image = \"\"\n",
    "                    \n",
    "                    img_tag = item.select_one(\"img\")\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        item_image = img_tag.get('src')\n",
    "                    \n",
    "                    menu_data.append({\n",
    "                        \"restaurant\": \"Burger King\",\n",
    "                        \"category\": category_name,\n",
    "                        \"name\": item_name,\n",
    "                        \"price\": item_price,\n",
    "                        \"description\": item_description,\n",
    "                        \"image_url\": item_image\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Scraped {len(menu_data)} items from Burger King menu\")\n",
    "            return menu_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Burger King menu: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scroll_page(self, pause_time=1.0):\n",
    "        \"\"\"Scroll down the page to ensure all dynamic content is loaded\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            # Wait to load page\n",
    "            time.sleep(pause_time)\n",
    "            \n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def save_as_json(self, data, filename):\n",
    "        \"\"\"Save data as JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "            logger.info(f\"Data successfully saved to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving JSON file: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def save_as_csv(self, data, filename):\n",
    "        \"\"\"Save data as CSV file\"\"\"\n",
    "        try:\n",
    "            if not data:\n",
    "                logger.warning(\"No data to save to CSV\")\n",
    "                return False\n",
    "                \n",
    "            # For CSV, we need to flatten the nutritional_info dictionary if it exists\n",
    "            flattened_data = []\n",
    "            for item in data:\n",
    "                flat_item = item.copy()\n",
    "                \n",
    "                # Handle nutritional_info if it exists\n",
    "                if \"nutritional_info\" in flat_item and isinstance(flat_item[\"nutritional_info\"], dict):\n",
    "                    for key, value in flat_item[\"nutritional_info\"].items():\n",
    "                        flat_item[f\"nutrition_{key}\"] = value\n",
    "                    del flat_item[\"nutritional_info\"]\n",
    "                \n",
    "                flattened_data.append(flat_item)\n",
    "            \n",
    "            # Extract column headers from all items to ensure we include all possible fields\n",
    "            all_fields = set()\n",
    "            for item in flattened_data:\n",
    "                all_fields.update(item.keys())\n",
    "            \n",
    "            fieldnames = sorted(list(all_fields))\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(flattened_data)\n",
    "            logger.info(f\"Data successfully saved to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving CSV file: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_restaurant(self, restaurant_name):\n",
    "        \"\"\"Scrape menu based on restaurant name\"\"\"\n",
    "        restaurant_name = restaurant_name.lower().strip()\n",
    "        \n",
    "        if \"a&w\" in restaurant_name or \"a & w\" in restaurant_name:\n",
    "            return self.scrape_aw()\n",
    "        elif \"mcdonald\" in restaurant_name:\n",
    "            return self.scrape_mcdonalds()\n",
    "        elif \"burger king\" in restaurant_name or \"burgerking\" in restaurant_name:\n",
    "            return self.scrape_burger_king()\n",
    "        else:\n",
    "            logger.error(f\"Unsupported restaurant: {restaurant_name}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.quit()\n",
    "            logger.info(\"WebDriver closed\")\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Scrape restaurant menus and save them in JSON and CSV formats.')\n",
    "    parser.add_argument('--restaurants', nargs='+', default=[\"A&W\", \"McDonalds\", \"Burger King\"],\n",
    "                        help='List of restaurants to scrape (default: A&W, McDonalds, Burger King)')\n",
    "    parser.add_argument('--output-dir', default=\"menu_data\",\n",
    "                        help='Directory to save the scraped data (default: menu_data)')\n",
    "    parser.add_argument('--no-headless', action='store_true',\n",
    "                        help='Run the browser in non-headless mode (visible)')\n",
    "    parser.add_argument('--proxy', help='Use a proxy server (format: ip:port)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup for using a proxy if specified\n",
    "    use_proxy = args.proxy is not None\n",
    "    \n",
    "    # Initialize the scraper\n",
    "    scraper = MenuScraper(\n",
    "        headless=not args.no_headless,\n",
    "        use_proxy=use_proxy,\n",
    "        proxy=args.proxy\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        all_menu_data = []\n",
    "        \n",
    "        for restaurant in args.restaurants:\n",
    "            logger.info(f\"Processing {restaurant} menu\")\n",
    "            menu_data = scraper.scrape_restaurant(restaurant)\n",
    "            \n",
    "            if menu_data:\n",
    "                # Save individual restaurant data\n",
    "                restaurant_name = restaurant.lower().replace(\" \", \"_\").replace(\"&\", \"and\")\n",
    "                output_base = os.path.join(args.output_dir, restaurant_name)\n",
    "                \n",
    "                scraper.save_as_json(menu_data, f\"{output_base}_menu.json\")\n",
    "                scraper.save_as_csv(menu_data, f\"{output_base}_menu.csv\")\n",
    "                \n",
    "                # Add to combined data\n",
    "                all_menu_data.extend(menu_data)\n",
    "        \n",
    "        # Save combined data if more than one restaurant was scraped\n",
    "        if len(args.restaurants) > 1 and all_menu_data:\n",
    "            output_base = os.path.join(args.output_dir, \"all_restaurants\")\n",
    "            scraper.save_as_json(all_menu_data, f\"{output_base}_menu.json\")\n",
    "            scraper.save_as_csv(all_menu_data, f\"{output_base}_menu.csv\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 06:19:10,156 - INFO - ChromeType not available, falling back to manual detection\n",
      "2025-03-09 06:19:10,156 - INFO - Chrome binary not found in common locations, installing Chrome...\n",
      "E: List directory /var/lib/apt/lists/partial is missing. - Acquire (13: Permission denied)\n",
      "2025-03-09 06:19:10,180 - WARNING - Could not locate Chrome binary explicitly. Trying default configuration.\n",
      "2025-03-09 06:19:10,181 - ERROR - Error setting up WebDriver: cannot access local variable 'ChromeDriverManager' where it is not associated with a value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'ChromeDriverManager' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 480\u001b[0m\n\u001b[1;32m    477\u001b[0m         scraper\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 444\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    443\u001b[0m \u001b[38;5;66;03m# Initialize the scraper\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m scraper \u001b[38;5;241m=\u001b[39m \u001b[43mMenuScraper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheadless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheadless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_proxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     all_menu_data \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[26], line 42\u001b[0m, in \u001b[0;36mMenuScraper.__init__\u001b[0;34m(self, headless, use_proxy, proxy)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproxy \u001b[38;5;241m=\u001b[39m proxy\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_agent \u001b[38;5;241m=\u001b[39m UserAgent()\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_driver\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 115\u001b[0m, in \u001b[0;36mMenuScraper.setup_driver\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate Chrome binary explicitly. Trying default configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# Use ChromeDriverManager to handle driver installation\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m service \u001b[38;5;241m=\u001b[39m Service(\u001b[43mChromeDriverManager\u001b[49m()\u001b[38;5;241m.\u001b[39minstall())\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mservice, options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[1;32m    117\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWebDriver initialized successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'ChromeDriverManager' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up logging\n",
    "log_directory = 'logs'\n",
    "os.makedirs(log_directory, exist_ok=True)\n",
    "log_file = f\"{log_directory}/menu_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_file),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class MenuScraper:\n",
    "    def __init__(self, headless=True, use_proxy=False, proxy=None):\n",
    "        self.headless = headless\n",
    "        self.use_proxy = use_proxy\n",
    "        self.proxy = proxy\n",
    "        self.user_agent = UserAgent()\n",
    "        self.setup_driver()\n",
    "        \n",
    "    def setup_driver(self):\n",
    "        \"\"\"Set up the Selenium WebDriver with Chrome\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            if self.headless:\n",
    "                chrome_options.add_argument(\"--headless=new\")\n",
    "            \n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--disable-extensions\")\n",
    "            chrome_options.add_argument(\"--disable-notifications\")\n",
    "            chrome_options.add_argument(\"--disable-popup-blocking\")\n",
    "            chrome_options.add_argument(f\"user-agent={self.user_agent.random}\")\n",
    "            \n",
    "            if self.use_proxy and self.proxy:\n",
    "                chrome_options.add_argument(f'--proxy-server={self.proxy}')\n",
    "                \n",
    "            # Find Chrome binary path based on operating system\n",
    "            system = platform.system()\n",
    "            chrome_found = False\n",
    "            \n",
    "            if system == \"Linux\":\n",
    "                # First, check if Chrome is already properly installed \n",
    "                try:\n",
    "                    # Try importing webdriver_manager's ChromeType for better detection\n",
    "                    from webdriver_manager.core.utils import ChromeType\n",
    "                    # Let webdriver_manager handle finding the Chrome binary for us\n",
    "                    from webdriver_manager.chrome import ChromeDriverManager\n",
    "                    \n",
    "                    # No need to specify chrome_options.binary_location when using this approach\n",
    "                    chrome_found = True\n",
    "                except ImportError:\n",
    "                    logger.info(\"ChromeType not available, falling back to manual detection\")\n",
    "                \n",
    "                if not chrome_found:\n",
    "                    # Common locations for Chrome binary on Linux\n",
    "                    possible_paths = [\n",
    "                        \"/usr/bin/google-chrome\",\n",
    "                        \"/usr/bin/google-chrome-stable\",\n",
    "                        \"/usr/bin/chromium\",\n",
    "                        \"/usr/bin/chromium-browser\",\n",
    "                        \"/snap/bin/chromium\",\n",
    "                        \"/snap/bin/google-chrome\",\n",
    "                    ]\n",
    "                    \n",
    "                    for path in possible_paths:\n",
    "                        if os.path.exists(path):\n",
    "                            chrome_options.binary_location = path\n",
    "                            logger.info(f\"Chrome binary found at: {path}\")\n",
    "                            chrome_found = True\n",
    "                            break\n",
    "\n",
    "            # If Chrome still not found, try to install it first\n",
    "            if system == \"Linux\" and not chrome_found:\n",
    "                logger.info(\"Chrome binary not found in common locations, installing Chrome...\")\n",
    "                try:\n",
    "                    # Try different approaches to install Chrome\n",
    "                    os.system(\"apt-get update && apt-get install -y google-chrome-stable\")\n",
    "                    if os.path.exists(\"/usr/bin/google-chrome-stable\"):\n",
    "                        chrome_options.binary_location = \"/usr/bin/google-chrome-stable\"\n",
    "                        chrome_found = True\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error installing Chrome: {str(e)}\")\n",
    "\n",
    "            # Final fallback - try to get ChromeDriver without specifying binary location\n",
    "            if not chrome_found:\n",
    "                logger.warning(\"Could not locate Chrome binary explicitly. Trying default configuration.\")\n",
    "                \n",
    "            # Use ChromeDriverManager to handle driver installation\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            logger.info(\"WebDriver initialized successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error setting up WebDriver: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def random_delay(self, min_sec=1, max_sec=5):\n",
    "        \"\"\"Add a random delay to mimic human behavior and avoid detection\"\"\"\n",
    "        delay = random.uniform(min_sec, max_sec)\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    def scrape_aw(self):\n",
    "        \"\"\"Scrape A&W menu\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting A&W menu scraping\")\n",
    "            menu_url = \"https://web.aw.ca/en/our-menu\"\n",
    "            self.driver.get(menu_url)\n",
    "            \n",
    "            # Wait for menu items to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".menu-category-container\"))\n",
    "            )\n",
    "            \n",
    "            # Allow time for dynamic content to load\n",
    "            self.random_delay(2, 4)\n",
    "            \n",
    "            # Scroll down to load all content\n",
    "            self.scroll_page()\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            menu_data = []\n",
    "            \n",
    "            # Extract menu categories\n",
    "            menu_categories = soup.select(\".menu-category-container\")\n",
    "            logger.info(f\"Found {len(menu_categories)} menu categories\")\n",
    "            \n",
    "            for category in menu_categories:\n",
    "                # Get category name\n",
    "                category_title_elem = category.select_one(\".menu-category-title\")\n",
    "                category_name = category_title_elem.get_text(strip=True) if category_title_elem else \"Uncategorized\"\n",
    "                logger.info(f\"Processing category: {category_name}\")\n",
    "                \n",
    "                # Extract menu items for this category\n",
    "                menu_items = category.select(\".menu-item\")\n",
    "                for item in menu_items:\n",
    "                    # Extract item name\n",
    "                    item_name_elem = item.select_one(\".menu-item-title\")\n",
    "                    item_name = item_name_elem.get_text(strip=True) if item_name_elem else \"Unknown\"\n",
    "                    \n",
    "                    # Extract item description\n",
    "                    item_desc_elem = item.select_one(\".menu-item-desc\")\n",
    "                    item_description = item_desc_elem.get_text(strip=True) if item_desc_elem else \"\"\n",
    "                    \n",
    "                    # Extract price (A&W might not show prices directly on the menu page)\n",
    "                    item_price_elem = item.select_one(\".menu-item-price\")\n",
    "                    item_price = item_price_elem.get_text(strip=True) if item_price_elem else \"Price not available online\"\n",
    "                    \n",
    "                    # Extract image URL if available\n",
    "                    item_image = \"\"\n",
    "                    img_tag = item.select_one(\"img\")\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        item_image = img_tag.get('src')\n",
    "                        \n",
    "                        # If it's a relative URL, make it absolute\n",
    "                        if item_image.startswith('/'):\n",
    "                            item_image = f\"https://web.aw.ca{item_image}\"\n",
    "                    \n",
    "                    # Add nutritional info if available\n",
    "                    nutritional_info = {}\n",
    "                    nutrition_elem = item.select_one(\".nutrition-info\")\n",
    "                    if nutrition_elem:\n",
    "                        nutrition_items = nutrition_elem.select(\".nutrition-item\")\n",
    "                        for n_item in nutrition_items:\n",
    "                            key_elem = n_item.select_one(\".nutrition-key\")\n",
    "                            value_elem = n_item.select_one(\".nutrition-value\")\n",
    "                            if key_elem and value_elem:\n",
    "                                key = key_elem.get_text(strip=True)\n",
    "                                value = value_elem.get_text(strip=True)\n",
    "                                nutritional_info[key] = value\n",
    "                    \n",
    "                    menu_data.append({\n",
    "                        \"restaurant\": \"A&W\",\n",
    "                        \"category\": category_name,\n",
    "                        \"name\": item_name,\n",
    "                        \"price\": item_price,\n",
    "                        \"description\": item_description,\n",
    "                        \"image_url\": item_image,\n",
    "                        \"nutritional_info\": nutritional_info\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Scraped {len(menu_data)} items from A&W menu\")\n",
    "            return menu_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping A&W menu: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scrape_mcdonalds(self):\n",
    "        \"\"\"Scrape McDonald's menu\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting McDonald's menu scraping\")\n",
    "            menu_url = \"https://www.mcdonalds.com/us/en-us/full-menu.html\"\n",
    "            self.driver.get(menu_url)\n",
    "            \n",
    "            # Wait for menu items to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".category-wrapper\"))\n",
    "            )\n",
    "            \n",
    "            # Allow time for dynamic content to load\n",
    "            self.random_delay(2, 4)\n",
    "            \n",
    "            # Scroll down to load all content\n",
    "            self.scroll_page()\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            menu_data = []\n",
    "            \n",
    "            # Extract menu categories\n",
    "            menu_categories = soup.select(\".category-wrapper\")\n",
    "            logger.info(f\"Found {len(menu_categories)} menu categories\")\n",
    "            \n",
    "            for category in menu_categories:\n",
    "                category_name = category.select_one(\"h2\").get_text(strip=True) if category.select_one(\"h2\") else \"Uncategorized\"\n",
    "                logger.info(f\"Processing category: {category_name}\")\n",
    "                \n",
    "                # Extract menu items for this category\n",
    "                menu_items = category.select(\".cmp-category-item\")\n",
    "                for item in menu_items:\n",
    "                    item_name = item.select_one(\".item-title\").get_text(strip=True) if item.select_one(\".item-title\") else \"Unknown\"\n",
    "                    item_price = item.select_one(\".item-price\").get_text(strip=True) if item.select_one(\".item-price\") else \"N/A\"\n",
    "                    item_description = item.select_one(\".item-description\").get_text(strip=True) if item.select_one(\".item-description\") else \"\"\n",
    "                    item_image = \"\"\n",
    "                    \n",
    "                    img_tag = item.select_one(\"img\")\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        item_image = img_tag.get('src')\n",
    "                    \n",
    "                    menu_data.append({\n",
    "                        \"restaurant\": \"McDonald's\",\n",
    "                        \"category\": category_name,\n",
    "                        \"name\": item_name,\n",
    "                        \"price\": item_price,\n",
    "                        \"description\": item_description,\n",
    "                        \"image_url\": item_image\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Scraped {len(menu_data)} items from McDonald's menu\")\n",
    "            return menu_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping McDonald's menu: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def scrape_burger_king(self):\n",
    "        \"\"\"Scrape Burger King menu\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Starting Burger King menu scraping\")\n",
    "            menu_url = \"https://www.bk.com/menu\"\n",
    "            self.driver.get(menu_url)\n",
    "            \n",
    "            # Wait for menu items to load\n",
    "            WebDriverWait(self.driver, 15).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".menuPage_menuCategory__Qbda1\"))\n",
    "            )\n",
    "            \n",
    "            # Allow time for dynamic content to load\n",
    "            self.random_delay(2, 4)\n",
    "            \n",
    "            # Scroll down to load all content\n",
    "            self.scroll_page()\n",
    "            \n",
    "            # Get page source and parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            menu_data = []\n",
    "            \n",
    "            # Extract menu categories\n",
    "            menu_categories = soup.select(\".menuPage_menuCategory__Qbda1\")\n",
    "            logger.info(f\"Found {len(menu_categories)} menu categories\")\n",
    "            \n",
    "            for category in menu_categories:\n",
    "                category_name = category.select_one(\"h2\").get_text(strip=True) if category.select_one(\"h2\") else \"Uncategorized\"\n",
    "                logger.info(f\"Processing category: {category_name}\")\n",
    "                \n",
    "                # Extract menu items for this category\n",
    "                menu_items = category.select(\".menuItem_wrapper__X_zY_\")\n",
    "                for item in menu_items:\n",
    "                    item_name = item.select_one(\".menuItem_name__on_cM\").get_text(strip=True) if item.select_one(\".menuItem_name__on_cM\") else \"Unknown\"\n",
    "                    item_price = item.select_one(\".menuItem_price__TPsSC\").get_text(strip=True) if item.select_one(\".menuItem_price__TPsSC\") else \"N/A\"\n",
    "                    item_description = item.select_one(\".menuItem_description__i5zkV\").get_text(strip=True) if item.select_one(\".menuItem_description__i5zkV\") else \"\"\n",
    "                    item_image = \"\"\n",
    "                    \n",
    "                    img_tag = item.select_one(\"img\")\n",
    "                    if img_tag and img_tag.get('src'):\n",
    "                        item_image = img_tag.get('src')\n",
    "                    \n",
    "                    menu_data.append({\n",
    "                        \"restaurant\": \"Burger King\",\n",
    "                        \"category\": category_name,\n",
    "                        \"name\": item_name,\n",
    "                        \"price\": item_price,\n",
    "                        \"description\": item_description,\n",
    "                        \"image_url\": item_image\n",
    "                    })\n",
    "            \n",
    "            logger.info(f\"Scraped {len(menu_data)} items from Burger King menu\")\n",
    "            return menu_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error scraping Burger King menu: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    def scroll_page(self, pause_time=1.0):\n",
    "        \"\"\"Scroll down the page to ensure all dynamic content is loaded\"\"\"\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            \n",
    "            # Wait to load page\n",
    "            time.sleep(pause_time)\n",
    "            \n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "    def save_as_json(self, data, filename):\n",
    "        \"\"\"Save data as JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(data, json_file, indent=4, ensure_ascii=False)\n",
    "            logger.info(f\"Data successfully saved to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving JSON file: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def save_as_csv(self, data, filename):\n",
    "        \"\"\"Save data as CSV file\"\"\"\n",
    "        try:\n",
    "            if not data:\n",
    "                logger.warning(\"No data to save to CSV\")\n",
    "                return False\n",
    "                \n",
    "            # For CSV, we need to flatten the nutritional_info dictionary if it exists\n",
    "            flattened_data = []\n",
    "            for item in data:\n",
    "                flat_item = item.copy()\n",
    "                \n",
    "                # Handle nutritional_info if it exists\n",
    "                if \"nutritional_info\" in flat_item and isinstance(flat_item[\"nutritional_info\"], dict):\n",
    "                    for key, value in flat_item[\"nutritional_info\"].items():\n",
    "                        flat_item[f\"nutrition_{key}\"] = value\n",
    "                    del flat_item[\"nutritional_info\"]\n",
    "                \n",
    "                flattened_data.append(flat_item)\n",
    "            \n",
    "            # Extract column headers from all items to ensure we include all possible fields\n",
    "            all_fields = set()\n",
    "            for item in flattened_data:\n",
    "                all_fields.update(item.keys())\n",
    "            \n",
    "            fieldnames = sorted(list(all_fields))\n",
    "            \n",
    "            with open(filename, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "                writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "                writer.writeheader()\n",
    "                writer.writerows(flattened_data)\n",
    "            logger.info(f\"Data successfully saved to {filename}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving CSV file: {str(e)}\")\n",
    "            return False\n",
    "    \n",
    "    def scrape_restaurant(self, restaurant_name):\n",
    "        \"\"\"Scrape menu based on restaurant name\"\"\"\n",
    "        restaurant_name = restaurant_name.lower().strip()\n",
    "        \n",
    "        if \"a&w\" in restaurant_name or \"a & w\" in restaurant_name:\n",
    "            return self.scrape_aw()\n",
    "        elif \"mcdonald\" in restaurant_name:\n",
    "            return self.scrape_mcdonalds()\n",
    "        elif \"burger king\" in restaurant_name or \"burgerking\" in restaurant_name:\n",
    "            return self.scrape_burger_king()\n",
    "        else:\n",
    "            logger.error(f\"Unsupported restaurant: {restaurant_name}\")\n",
    "            return []\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the WebDriver\"\"\"\n",
    "        if hasattr(self, 'driver'):\n",
    "            self.driver.quit()\n",
    "            logger.info(\"WebDriver closed\")\n",
    "\n",
    "def main():\n",
    "    if 'ipykernel' in sys.argv[0]:\n",
    "        restaurants = [\"A&W\", \"McDonalds\", \"Burger King\"]\n",
    "        output_dir = \"menu_data\"\n",
    "        headless = True\n",
    "        proxy = None\n",
    "        use_proxy = False\n",
    "\n",
    "    else: \n",
    "        parser = argparse.ArgumentParser(description='Scrape restaurant menus and save them in JSON and CSV formats.')\n",
    "        parser.add_argument('--restaurants', nargs='+', default=[\"A&W\", \"McDonalds\", \"Burger King\"],\n",
    "                        help='List of restaurants to scrape (default: A&W, McDonalds, Burger King)')\n",
    "        parser.add_argument('--output-dir', default=\"menu_data\",\n",
    "                        help='Directory to save the scraped data (default: menu_data)')\n",
    "        parser.add_argument('--no-headless', action='store_true',\n",
    "                        help='Run the browser in non-headless mode (visible)')\n",
    "        parser.add_argument('--proxy', help='Use a proxy server (format: ip:port)')\n",
    "    \n",
    "        args = parser.parse_args()\n",
    "        restaurants = args.restaurants\n",
    "        output_dir = args.output_dir\n",
    "        headless = not args.no_headless\n",
    "        proxy = args.proxy\n",
    "        use_proxy = proxy is not None\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Initialize the scraper\n",
    "    scraper = MenuScraper(\n",
    "        headless=headless,\n",
    "        use_proxy=use_proxy,\n",
    "        proxy=proxy\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        all_menu_data = []\n",
    "        all_menu_data = []\n",
    "        \n",
    "        for restaurant in restaurants:\n",
    "            logger.info(f\"Processing {restaurant} menu\")\n",
    "            menu_data = scraper.scrape_restaurant(restaurant)\n",
    "            \n",
    "            if menu_data:\n",
    "                # Save individual restaurant data\n",
    "                restaurant_name = restaurant.lower().replace(\" \", \"_\").replace(\"&\", \"and\")\n",
    "                output_base = os.path.join(output_dir, restaurant_name)\n",
    "                \n",
    "                scraper.save_as_json(menu_data, f\"{output_base}_menu.json\")\n",
    "                scraper.save_as_csv(menu_data, f\"{output_base}_menu.csv\")\n",
    "                \n",
    "                # Add to combined data\n",
    "                all_menu_data.extend(menu_data)\n",
    "        \n",
    "        # Save combined data if more than one restaurant was scraped\n",
    "        if len(restaurants) > 1 and all_menu_data:\n",
    "            output_base = os.path.join(output_dir, \"all_restaurants\")\n",
    "            scraper.save_as_json(all_menu_data, f\"{output_base}_menu.json\")\n",
    "            scraper.save_as_csv(all_menu_data, f\"{output_base}_menu.csv\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main execution: {str(e)}\")\n",
    "    finally:\n",
    "        scraper.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing contents of: /workspaces/test_demo/Menu-scraping/menu_data\n",
      "  Directory menu_data does not exist!\n"
     ]
    }
   ],
   "source": [
    "# Check what files were created in the output directory\n",
    "import os\n",
    "\n",
    "def list_directory_contents(directory):\n",
    "    print(f\"Listing contents of: {os.path.abspath(directory)}\")\n",
    "    if os.path.exists(directory):\n",
    "        files = os.listdir(directory)\n",
    "        if files:\n",
    "            for file in files:\n",
    "                file_path = os.path.join(directory, file)\n",
    "                file_size = os.path.getsize(file_path) / 1024  # Size in KB\n",
    "                print(f\"   {file} ({file_size:.2f} KB)\")\n",
    "        else:\n",
    "            print(\"  (directory is empty)\")\n",
    "    else:\n",
    "        print(f\"  Directory {directory} does not exist!\")\n",
    "\n",
    "list_directory_contents(\"menu_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
